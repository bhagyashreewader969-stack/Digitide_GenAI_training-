# Local LLM Experiment (Ollama / LLaMA 3.2)

## Steps Performed

1. **Installation**
   - Installed Ollama or local LLM: [ ] Yes / [ ] No
   - Command used: `curl -fsSL https://ollama.com/install.sh | sh` (Linux/macOS) OR installer (Windows)

2. **Run Prompt**
   - Model used: llama3.2 (or other)  
   - Prompt: `Write a short poem about AI.`  
   - Example response:
     ```
     (Paste the modelâ€™s generated output here)
     ```

3. **Response Time**
   - Command: `time ollama run llama3.2 "Write a short poem about AI."`
   - Result:
     ```
     real    XmYs
     user    XmYs
     sys     XmYs
     ```

4. **Troubleshooting**
   - Issues faced:  
     - [ ] Installation failed  
     - [ ] Model crashed  
     - [ ] Out of memory  
     - [ ] Other: ____________  

   - Fixes applied:
     ```
     (Document fixes here)
     ```
